import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
import re
from datasets import load_dataset
import psycopg
import sys
from dotenv import load_dotenv
# ----------------------------------------------------
# 1. æ•°æ®åº“é…ç½®
# ----------------------------------------------------
DB_CONFIG = {
    "host": os.getenv("PG_HOST"),
    "dbname": os.getenv("PG_DBNAME"),
    "user": os.getenv("PG_USER"),
    "password": os.getenv("PG_PASSWORD"),
    "port": int(os.getenv("PG_PORT", 5432))
}
TABLE_NAME = "new_num_sentences"

# ----------------------------------------------------
# 2. å…¶ä»–é…ç½®
# ----------------------------------------------------
DATASET_ID = "opencsg/Fineweb-Edu-Chinese-V2.1"
MAX_DOCS_FOR_TEST = 10000 
COMMIT_INTERVAL = 50  # æ¯å¤„ç† 50 ä¸ªåŒ¹é…å¥å­æäº¤ä¸€æ¬¡äº‹åŠ¡
chinese_num_pattern = re.compile(r'[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡äº¿]')

# ----------------------------------------------------
# 3. æ ¸å¿ƒå‡½æ•°
# ----------------------------------------------------

def split_sentences(text):
    # ä¿æŒåˆ†å¥é€»è¾‘ä¸å˜
    sentences = re.split(r'([ã€‚ï¼ï¼Ÿ\n]+)', text)
    new_sents = []
    for i in range(0, len(sentences) - 1, 2):
        new_sents.append(sentences[i] + sentences[i+1])
    if len(sentences) % 2 != 0 and sentences[-1]:
        new_sents.append(sentences[-1])
    return [s.strip() for s in new_sents if s.strip()]

def setup_database(conn):
    """åˆ›å»ºæˆ–ç¡®ä¿ç›®æ ‡è¡¨å­˜åœ¨ï¼Œå¹¶å®šä¹‰è‡ªå¢ ID å’Œå†…å®¹å­—æ®µã€‚"""
    print(f"--- æ­£åœ¨è®¾ç½® PostgreSQL æ•°æ®åº“è¡¨: {TABLE_NAME} ---")
    
    # ä½¿ç”¨ cursor æäº¤ SQL å‘½ä»¤
    with conn.cursor() as cur:
        create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS {TABLE_NAME} (
            id SERIAL PRIMARY KEY,
            content TEXT NOT NULL
        );
        """
        cur.execute(create_table_sql)
        conn.commit()
    print("--- æ•°æ®åº“è¡¨è®¾ç½®å®Œæˆã€‚ ---")


def process_dataset_and_save_to_sql():
    # å°è¯•å»ºç«‹æ•°æ®åº“è¿æ¥
    conn = None
    try:
        conn = psycopg.connect(**DB_CONFIG)
        setup_database(conn)
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥æˆ–è¡¨åˆ›å»ºå¤±è´¥: {e}")
        return

    print(f"æ­£åœ¨è¿æ¥ Hugging Face åŠ è½½æ•°æ®é›†: {DATASET_ID} (æµå¼æ¨¡å¼)...")
    
    try:
        dataset = load_dataset(DATASET_ID, split="train", streaming=True)
    except Exception as e:
        print(f"åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æˆæƒ: {e}")
        conn.close()
        return

    print(f"--- æ•°æ®é›†å¯¹è±¡å·²åˆ›å»ºï¼Œå‡†å¤‡å¼€å§‹è¿­ä»£æµ... ---")
    print(f"å¼€å§‹å¤„ç†... åŒ¹é…åˆ°çš„å¥å­å°†ä¿å­˜åˆ° PostgreSQL è¡¨: {TABLE_NAME}")
    
    match_count = 0
    insert_sql = f"INSERT INTO {TABLE_NAME} (content) VALUES (%s);"

    try:
        with conn.cursor() as cur:
            for i, sample in enumerate(dataset):
                doc_id = sample.get('id', 'unknown')
                content = sample.get('text', '') 
                
                # å®æ—¶è¿›åº¦æ›´æ–°
                if i % 500 == 0 and i > 0:
                    sys.stdout.write(f"--- è¿›åº¦æ›´æ–°: å·²å¤„ç† {i} ç¯‡æ–‡æ¡£ã€‚å·²ä¿å­˜ {match_count} ä¸ªåŒ¹é…å¥å­ã€‚ ---\r")
                    sys.stdout.flush()

                if not content:
                    continue

                # åˆ†å¥
                sentences = split_sentences(content)
                
                # ç®€å•æ­£åˆ™ç­›é€‰
                for sentence in sentences:
                    if chinese_num_pattern.search(sentence):
                        
                        clean_sent = sentence.strip()
                        
                        # ğŸŒŸ æ ¸å¿ƒåŠŸèƒ½ï¼šæ’å…¥æ•°æ®åº“
                        cur.execute(insert_sql, (clean_sent,))
                        match_count += 1
                        
                        # æ‰¹é‡æäº¤ (æ€§èƒ½å…³é”®)
                        if match_count % COMMIT_INTERVAL == 0:
                            conn.commit()
                            
                            # æ‰“å°åŒ¹é…ç»“æœ (é¿å…è¢«è¿›åº¦æ¡è¦†ç›–)
                            print(f"\n[{doc_id}] å‘ç°å¹¶å·²æäº¤ (æ€»æ•°: {match_count}): {clean_sent}") 

                # å¼ºåˆ¶åœæ­¢é€»è¾‘ (æµ‹è¯•é˜¶æ®µ)
                if i >= MAX_DOCS_FOR_TEST: 
                     break
        
        # å¾ªç¯ç»“æŸï¼šæäº¤æ‰€æœ‰å‰©ä½™çš„äº‹åŠ¡
        conn.commit()
        print(f"\n--- è¿­ä»£å¾ªç¯ç»“æŸ ---")

    except Exception as e:
        print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œæ•°æ®æœªå®Œå…¨ä¿å­˜ã€‚é”™è¯¯: {e}")
        # å‘ç”Ÿé”™è¯¯æ—¶å°è¯•å›æ»š
        if conn:
            conn.rollback()

    finally:
        if conn:
            conn.close()
            print(f"æ•°æ®åº“è¿æ¥å·²å…³é—­ã€‚")
            
    print(f"å¤„ç†å®Œæˆã€‚å…±æ‰¾åˆ°å¹¶ä¿å­˜ {match_count} ä¸ªåŒ…å«ä¸­æ–‡æ•°å­—çš„å¥å­ã€‚")

if __name__ == "__main__":
    process_dataset_and_save_to_sql()