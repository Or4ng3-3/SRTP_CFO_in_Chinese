#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os
from dotenv import load_dotenv
import json
import time
import logging
import requests
import psycopg
import re
from psycopg.rows import dict_row
from concurrent.futures import ThreadPoolExecutor, as_completed

load_dotenv()
# ---------------------------
# é…ç½®
# ---------------------------
TABLE_NAME = "new_num_sentences"
NEW_TABLE = "number_level_analysis"  # æ–°è¡¨åï¼šæ•°è¯çº§åˆ«çš„åˆ†æ

DB_CONFIG = {
    "host": os.getenv("PG_HOST"),
    "dbname": os.getenv("PG_DBNAME"),
    "user": os.getenv("PG_USER"),
    "password": os.getenv("PG_PASSWORD"),
    "port": int(os.getenv("PG_PORT", 5432))
}

BASE_URL = "https://api.vectorengine.ai"
API_KEY = os.getenv("VECTORENGINE_API_KEY")
GEMINI_MODEL = "gemini-2.5-flash"

REQUEST_TIMEOUT = 30
MODEL_RETRIES = 2

# å¹¶å‘ & batch å‚æ•°
BATCH_SIZE = 20  # å¢åŠ batch sizeï¼Œå› ä¸ºç°åœ¨æ˜¯æ•°è¯çº§åˆ«
MAX_WORKERS = 6

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s"
)

# ---------------------------
# æ•°è¯æå–å‡½æ•° â­ æ ¸å¿ƒæ–°å¢
# ---------------------------
def extract_chinese_numbers(text):
    """
    æå–å¥å­ä¸­æ‰€æœ‰çš„ä¸­æ–‡æ•°è¯åŠå…¶ä½ç½®
    è¿”å›: [{'text': 'ä¸‰', 'start': 5, 'end': 6}, ...]
    """
    # åŒ¹é…è¿ç»­çš„ä¸­æ–‡æ•°å­—å­—ç¬¦
    pattern = r'[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡äº¿]+'
    matches = []
    for match in re.finditer(pattern, text):
        matches.append({
            'text': match.group(),
            'start': match.start(),
            'end': match.end()
        })
    return matches

# ---------------------------
# æ¨¡å‹è°ƒç”¨
# ---------------------------
def call_gemini_json(prompt: str, temperature: float = 0.1, retry: int = MODEL_RETRIES):
    url = f"{BASE_URL}/v1/chat/completions"
    headers = {
        "Accept": "application/json",
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": GEMINI_MODEL,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": temperature,
        "response_format": {"type": "json_object"}
    }

    last_err = None
    for i in range(retry):
        try:
            r = requests.post(url, headers=headers, json=payload, timeout=REQUEST_TIMEOUT)
            r.raise_for_status()
            data = r.json()
            msg = data["choices"][0]["message"]["content"]
            if isinstance(msg, dict):
                return msg
            return json.loads(msg)
        except Exception as e:
            last_err = e
            logging.warning(f"æ¨¡å‹è¯·æ±‚å¤±è´¥ï¼ˆ{i+1}/{retry}ï¼‰ï¼š{e}")
            time.sleep(0.5 * (i + 1))
    raise last_err

# ---------------------------
# DB åˆå§‹åŒ– â­ ä¿®æ”¹è¡¨ç»“æ„
# ---------------------------
def ensure_new_table(conn):
    """åˆ›å»ºæ•°è¯çº§åˆ«çš„åˆ†æè¡¨"""
    with conn.cursor() as cur:
        cur.execute(f"""
        CREATE TABLE IF NOT EXISTS {NEW_TABLE} (
            id BIGSERIAL PRIMARY KEY,
            source_id BIGINT NOT NULL,           -- æ¥æºå¥å­ID
            sentence TEXT NOT NULL,              -- å®Œæ•´å¥å­
            number_text TEXT NOT NULL,           -- æå–çš„æ•°è¯æ–‡æœ¬
            number_start INT NOT NULL,           -- æ•°è¯èµ·å§‹ä½ç½®
            number_end INT NOT NULL,             -- æ•°è¯ç»“æŸä½ç½®
            number_type INTEGER DEFAULT NULL,    -- 0=åŸºæ•°/1=åºæ•°/2=å›ºå®šçŸ­è¯­/3=æ— æ•°å­—å«ä¹‰
            created_at TIMESTAMPTZ DEFAULT NOW()
        );
        """)
        # åˆ›å»ºç´¢å¼•åŠ é€ŸæŸ¥è¯¢
        cur.execute(f"""
        CREATE INDEX IF NOT EXISTS idx_{NEW_TABLE}_source 
        ON {NEW_TABLE}(source_id);
        """)
        cur.execute(f"""
        CREATE INDEX IF NOT EXISTS idx_{NEW_TABLE}_type 
        ON {NEW_TABLE}(number_type);
        """)
        conn.commit()

def extract_and_insert_numbers(conn):
    """
    ä» all_num_sentences è¯»å–ä¸å«'ç¬¬'çš„å¥å­ï¼Œ
    æå–æ¯ä¸ªæ•°è¯ï¼Œæ’å…¥åˆ°æ–°è¡¨
    """
    logging.info("å¼€å§‹æå–æ•°è¯å¹¶æ’å…¥æ–°è¡¨...")
    
    with conn.cursor(row_factory=dict_row) as cur:
        # è¯»å–ä¸å«'ç¬¬'çš„å¥å­
        cur.execute(f"""
            SELECT id, content FROM {TABLE_NAME}
            WHERE content NOT LIKE '%ç¬¬%'
        """)
        sentences = cur.fetchall()
        
        logging.info(f"æ‰¾åˆ° {len(sentences)} æ¡ä¸å«'ç¬¬'çš„å¥å­")
        
        insert_count = 0
        for row in sentences:
            sentence_id = row['id']
            sentence_text = row['content']
            
            # æå–æ‰€æœ‰æ•°è¯
            numbers = extract_chinese_numbers(sentence_text)
            
            # ä¸ºæ¯ä¸ªæ•°è¯æ’å…¥ä¸€æ¡è®°å½•
            for num_info in numbers:
                cur.execute(f"""
                    INSERT INTO {NEW_TABLE} 
                    (source_id, sentence, number_text, number_start, number_end)
                    VALUES (%s, %s, %s, %s, %s)
                """, (
                    sentence_id,
                    sentence_text,
                    num_info['text'],
                    num_info['start'],
                    num_info['end']
                ))
                insert_count += 1
            
            if insert_count % 1000 == 0:
                conn.commit()
                logging.info(f"å·²æ’å…¥ {insert_count} æ¡æ•°è¯è®°å½•...")
        
        conn.commit()
        logging.info(f"âœ… æ•°è¯æå–å®Œæˆï¼Œå…±æ’å…¥ {insert_count} æ¡è®°å½•")

def fetch_rows_to_label(conn):
    """è·å–éœ€è¦æ ‡æ³¨çš„æ•°è¯è®°å½•"""
    with conn.cursor(row_factory=dict_row) as cur:
        cur.execute(f"""
            SELECT id, sentence, number_text, number_start, number_end
            FROM {NEW_TABLE}
            WHERE number_type IS NULL
            ORDER BY id
        """)
        rows = cur.fetchall()
    logging.info(f"æŸ¥è¯¢åˆ° {len(rows)} æ¡æœªæ ‡æ³¨çš„æ•°è¯è®°å½•ã€‚")
    return rows

# ---------------------------
# Prompt æ„é€  â­ é‡æ–°è®¾è®¡
# ---------------------------
def build_batch_prompt(rows):
    """
    æ„é€ æ‰¹é‡åˆ¤æ–­prompt
    rows: [{'id': 1, 'sentence': 'ä¸‰æ¥¼æœ‰ä¸€ç™¾äºº', 'number_text': 'ä¸‰', 'number_start': 0}, ...]
    """
    items = []
    for r in rows:
        items.append({
            "id": r["id"],
            "sentence": r["sentence"],
            "number": r["number_text"],
            "position": r["number_start"]
        })

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸­æ–‡æ•°è¯è¯­ä¹‰åˆ†æä¸“å®¶ã€‚æˆ‘ä¼šç»™ä½ ä¸€ä¸ªJSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
- id: è®°å½•ID
- sentence: å®Œæ•´çš„ä¸­æ–‡å¥å­
- number: å¥å­ä¸­æå–çš„æ•°è¯æ–‡æœ¬
- position: æ•°è¯åœ¨å¥å­ä¸­çš„èµ·å§‹ä½ç½®

è¯·åˆ¤æ–­æ¯ä¸ªæ•°è¯åœ¨å…¶å¥å­ä¸­çš„è¯­ä¹‰ç±»å‹ï¼Œè¿”å›æ•´æ•°ï¼š
- 0: åŸºæ•°å«ä¹‰ï¼ˆè¡¨ç¤ºæ•°é‡ï¼Œå¦‚"ä¸€ç™¾äºº"çš„"ä¸€ç™¾"ï¼‰
- 1: åºæ•°å«ä¹‰ï¼ˆè¡¨ç¤ºé¡ºåºï¼Œå¦‚"ä¸‰æ¥¼"çš„"ä¸‰"ã€"äºŒæœˆ"çš„"äºŒ"ï¼‰
- 2: æ— æ•°å­—å«ä¹‰ï¼ˆå¦‚"ä¸€å‘"çš„"ä¸€"ã€"ä¸‡ä¸€"çš„"ä¸‡", è¿™ä¸€ç±»çš„è¡¨ç°å½¢å¼å¤šä¸ºçŸ­è¯­ï¼Œä½†å¹¶éæ‰€æœ‰çŸ­è¯­ä¸­çš„æ•°è¯éƒ½æ²¡æœ‰æ•°å­—å«ä¹‰ï¼Œæ¯”å¦‚è¯´"åŒä¸€"çš„ä¸€æ˜¯ä¸€ä¸ªçš„æ„æ€ï¼‰

âš ï¸ é‡è¦æç¤ºï¼š
1. å³ä½¿æ•°è¯çœ‹èµ·æ¥æ˜¯åŸºæ•°è¯å½¢å¼ï¼Œä¹Ÿå¯èƒ½è¡¨è¾¾åºæ•°å«ä¹‰
2. æ³¨æ„åŒºåˆ†"ä¸‰ä¸ªè‹¹æœ"ï¼ˆåŸºæ•°ï¼‰å’Œ"ä¸‰æ¥¼"ï¼ˆåºæ•°ï¼‰
3. æ¯ä¸ªæ•°è¯ç‹¬ç«‹åˆ¤æ–­ï¼ŒåŒä¸€å¥å­ä¸­ä¸åŒæ•°è¯å¯èƒ½æœ‰ä¸åŒå«ä¹‰

è¯·è¿”å›JSONæ ¼å¼ï¼š{{"results": [{{"id": 1, "type": 0}}, {{"id": 2, "type": 1}}, ...]}}

è¾“å…¥æ•°æ®ï¼š
{json.dumps(items, ensure_ascii=False, indent=2)}
"""
    return prompt

# ---------------------------
# å­çº¿ç¨‹ä»»åŠ¡
# ---------------------------
def process_batch_api_task(batch_rows):
    """å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡çš„APIè°ƒç”¨"""
    ids = [r["id"] for r in batch_rows]
    try:
        logging.info(f"ğŸš€ å¤„ç†æ‰¹æ¬¡: ids={ids[:5]}{'...' if len(ids) > 5 else ''} (å…±{len(ids)}æ¡)")
        
        prompt = build_batch_prompt(batch_rows)
        result = call_gemini_json(prompt)
        
        # æ‰“å°éƒ¨åˆ†è¿”å›ç»“æœç”¨äºè°ƒè¯•
        result_preview = json.dumps(result, ensure_ascii=False)[:300]
        logging.info(f"ğŸ“¥ æ¨¡å‹è¿”å›é¢„è§ˆ: {result_preview}...")
        
        return batch_rows, result
        
    except Exception as e:
        logging.error(f"âŒ æ‰¹æ¬¡å¤„ç†å¤±è´¥ ids={ids[:5]}...: {e}")
        return batch_rows, None

# ---------------------------
# ä¸»æµç¨‹
# ---------------------------
def process_all():
    conn = None
    try:
        conn = psycopg.connect(**DB_CONFIG)

        # 1. ç¡®ä¿æ–°è¡¨å­˜åœ¨
        ensure_new_table(conn)
        
        # 2. æå–æ•°è¯å¹¶æ’å…¥ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶æ‰§è¡Œï¼‰
        with conn.cursor() as cur:
            cur.execute(f"SELECT COUNT(*) as cnt FROM {NEW_TABLE}")
            count = cur.fetchone()[0]
            
        if count == 0:
            extract_and_insert_numbers(conn)
        else:
            logging.info(f"è¡¨ä¸­å·²æœ‰ {count} æ¡è®°å½•ï¼Œè·³è¿‡æ•°è¯æå–æ­¥éª¤")

        # 3. è·å–å¾…æ ‡æ³¨çš„è®°å½•
        rows = fetch_rows_to_label(conn)
        if not rows:
            logging.info("âœ… æ— å¾…å¤„ç†ä»»åŠ¡ï¼Œç»“æŸã€‚")
            return

        # 4. åˆ†æ‰¹å¤„ç†
        batches = [rows[i:i + BATCH_SIZE] for i in range(0, len(rows), BATCH_SIZE)]
        logging.info(
            f"ğŸ”§ å¹¶å‘é…ç½®ï¼šæ€»è®°å½•={len(rows)}, æ‰¹æ¬¡æ•°={len(batches)}, "
            f"BATCH_SIZE={BATCH_SIZE}, MAX_WORKERS={MAX_WORKERS}"
        )

        with conn.cursor() as cur, ThreadPoolExecutor(max_workers=MAX_WORKERS) as pool:
            futures = [pool.submit(process_batch_api_task, b) for b in batches]

            processed = 0
            failed = 0
            
            for fut in as_completed(futures):
                batch_rows, api_result = fut.result()

                # å¤„ç†å¤±è´¥æƒ…å†µ
                if api_result is None:
                    for r in batch_rows:
                        cur.execute(
                            f"UPDATE {NEW_TABLE} SET number_type=-1 WHERE id=%s",
                            (r["id"],)
                        )
                        failed += 1
                    conn.commit()
                    logging.warning(f"âš ï¸ æ‰¹æ¬¡å¤±è´¥ï¼Œæ ‡è®°ä¸º-1")
                    continue

                # è§£æç»“æœ
                results = api_result.get("results", [])
                if not isinstance(results, list):
                    logging.error(f"âŒ è¿”å›æ ¼å¼é”™è¯¯: {api_result}")
                    for r in batch_rows:
                        cur.execute(
                            f"UPDATE {NEW_TABLE} SET number_type=-1 WHERE id=%s",
                            (r["id"],)
                        )
                        failed += 1
                    conn.commit()
                    continue

                # æ›´æ–°æ•°æ®åº“
                id_map = {r["id"]: r for r in batch_rows}
                for item in results:
                    rid = item.get("id")
                    try:
                        num_type = int(item.get("type"))
                        if num_type not in (0, 1, 2):
                            num_type = -1
                    except Exception:
                        num_type = -1

                    if rid in id_map:
                        cur.execute(
                            f"UPDATE {NEW_TABLE} SET number_type=%s WHERE id=%s",
                            (num_type, rid)
                        )
                        processed += 1

                conn.commit()
                logging.info(
                    f"ğŸ’¾ æ‰¹æ¬¡å®Œæˆ: æˆåŠŸ{processed}/{len(rows)}, å¤±è´¥{failed}"
                )

        logging.info(f"ğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆï¼æˆåŠŸ: {processed}, å¤±è´¥: {failed}")

        # 5. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        with conn.cursor(row_factory=dict_row) as cur:
            cur.execute(f"""
                SELECT 
                    number_type,
                    COUNT(*) as count,
                    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage
                FROM {NEW_TABLE}
                WHERE number_type IS NOT NULL
                GROUP BY number_type
                ORDER BY number_type
            """)
            stats = cur.fetchall()
            
            logging.info("\n" + "="*50)
            logging.info("ğŸ“Š æ•°è¯ç±»å‹ç»Ÿè®¡:")
            logging.info("="*50)
            type_names = {
                -1: "å¤„ç†å¤±è´¥",
                0: "åŸºæ•°å«ä¹‰",
                1: "åºæ•°å«ä¹‰",
                2: "å›ºå®šçŸ­è¯­"
            }
            for row in stats:
                type_name = type_names.get(row['number_type'], 'æœªçŸ¥')
                logging.info(
                    f"  {type_name:10s}: {row['count']:6d} æ¡ ({row['percentage']:5.2f}%)"
                )
            logging.info("="*50)

    except Exception:
        logging.exception("âŒ ä¸»æµç¨‹å¼‚å¸¸")
        if conn:
            conn.rollback()
    finally:
        if conn:
            conn.close()
            logging.info("æ•°æ®åº“è¿æ¥å·²å…³é—­ã€‚")

# ---------------------------
if __name__ == "__main__":
    process_all()